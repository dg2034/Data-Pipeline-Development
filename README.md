# Data-Pipeline-Development

*COMPANY*: CODTECH IT SOLUTIONS

*NAME*: DEEP GHEEWALA

*INTERN ID*: CT04DF578

*DOMAIN*: DATA SCIENCE

*DURATION*: 4 WEEKS

*MENTOR*: NEELA SANTOSH

# ðŸ“Š Data Preprocessing ETL Pipeline using Pandas and Scikit-learn

This project is part of the **CODTECH Virtual Internship in Data Science** under **Task 1**, which focuses on building a simple yet powerful **ETL (Extract, Transform, Load)** pipeline using Python. The goal is to automate the core data preprocessing tasks commonly used in data science workflows using **Pandas** and **Scikit-learn**.

---

## ðŸš€ Project Objective

The main objective of this task is to demonstrate the ability to clean and transform raw data into a usable format for analysis or modeling. The project walks through the entire ETL process using the **Titanic dataset**, a well-known dataset in the data science community.

This task is designed to reflect real-world preprocessing scenarios, including missing value handling, feature encoding, and data scaling. These are essential steps in preparing data for machine learning or analytics tasks.

---

## ðŸ›  Tools and Libraries Used

* **Python**: The core programming language used for this project.
* **Pandas**: For data loading, exploration, and manipulation.
* **Scikit-learn**: For preprocessing tasks such as imputation, encoding, and feature scaling.
* **Jupyter Notebook**: Used to present and execute the code in a clean, step-by-step format with outputs and explanations.

---

## ðŸ“‚ Project Structure

```
.
â”œâ”€â”€ Titanic-Dataset.csv
â”œâ”€â”€ task_1.ipynb
â””â”€â”€ README.md
```

* `Titanic-Dataset.csv`: The raw dataset used in this project.
* `task_1.ipynb`: A Jupyter Notebook that contains the entire ETL pipeline including code, outputs, and comments.
* `README.md`: This file, which explains the project setup, purpose, and usage.

---

## ðŸ”„ ETL Process Overview

### 1. Extract

* Load the raw Titanic dataset directly from the current working directory using Pandas.
* Perform initial inspection and exploratory data analysis (EDA).

### 2. Transform

* Handle missing data using strategies like mean imputation for numerical columns.
* Encode categorical features such as "Sex" and "Embarked" using Label Encoding or One-Hot Encoding.
* Normalize numerical features like "Age" and "Fare" using StandardScaler for consistency.

### 3. Load

* The cleaned data is stored in a DataFrame and can be optionally saved. However, no external output file is included in this submission, as the primary focus is on building and understanding the pipeline.

---

## ðŸ’¡ Key Features

* Entire ETL process is contained in one notebook for easy understanding.
* Modular and well-commented code suitable for beginners and intermediate learners.
* Covers essential preprocessing steps that are required before applying machine learning models.
* Can be easily adapted for other datasets with minimal modifications.

---

## ðŸ“Œ How to Use

1. Clone this repository or download the files.
2. Make sure `Titanic-Dataset.csv` is in the same folder as the notebook.
3. Open and run `task_1.ipynb` using Jupyter Notebook, Google Colab, or VS Code with Jupyter extension.
4. Follow the step-by-step code and outputs to understand the ETL process.

---

## ðŸ“š Learning Outcomes

Through this project, I gained hands-on experience in:

* Automating data preprocessing workflows
* Using Scikit-learn tools like `SimpleImputer`, `LabelEncoder`, and `StandardScaler`
* Applying foundational data transformation techniques in real-world datasets

---

## ðŸ“§ Contact

Feel free to reach out for feedback or collaboration:

* Name: Deep Gheewala
* Email: deepgheewala04@gmail.com
* LinkedIn: https://www.linkedin.com/in/deep-gheewala-a1747a259/
